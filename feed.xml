<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://wongkj12.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://wongkj12.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-07-27T21:17:48+00:00</updated><id>https://wongkj12.github.io/feed.xml</id><title type="html">wongkj12</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">CLRS 16.3: Huffman codes</title><link href="https://wongkj12.github.io/blog/2023/huffman/" rel="alternate" type="text/html" title="CLRS 16.3: Huffman codes" /><published>2023-07-27T16:03:00+00:00</published><updated>2023-07-27T16:03:00+00:00</updated><id>https://wongkj12.github.io/blog/2023/huffman</id><content type="html" xml:base="https://wongkj12.github.io/blog/2023/huffman/"><![CDATA[<p>The section introduces Huffman coding, a type of code used for data compression.</p>

<p>Suppose we want to encode a long string of characters by encoding each character as a unique binary string. How can we minimise the number of bits? Here is an example of how we might encode each character:</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>a</th>
      <th>b</th>
      <th>c</th>
      <th>d</th>
      <th>e</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>freq</td>
      <td>30</td>
      <td>12</td>
      <td>24</td>
      <td>6</td>
      <td>3</td>
    </tr>
    <tr>
      <td>code</td>
      <td>0</td>
      <td>110</td>
      <td>10</td>
      <td>1110</td>
      <td>1111</td>
    </tr>
  </tbody>
</table>

<p><br />
The total number of bits required would be \(30*(1) + 12*(3) + 24*(2) + 6*(4) + 3*(4)\).</p>

<p>The encoding above is an example of a Huffman code, which are also <em>prefix codes</em>. Prefix codes are codes in which no codeword is also a prefix of some other codeword. This makes it simple to decode, as the codeword that begins an encoded file will be unambiguous.</p>

<hr />

<p><strong>Note on prefix codes</strong></p>

<p>The book claims without proof the interesting fact that “a prefix code can always achieve the optimal data compression among any character code”. According to <a href="https://en.wikipedia.org/wiki/Prefix_code">Wikipedia</a>, I think they refer to the fact that “for any uniquely decodable code there is a prefix code that has the same code word lengths”. This is characterised by the <em>Kraft inequality</em> - in an earlier <a href="https://wongkj12.github.io/blog/2023/treeproblems/">post</a> an exercise from Appendix B asks us to prove a weak version of it, but here is the full statement: Given any uniquely decodable code over an alphabet of size \(r\) with codeword lengths \(l_i\), then \(\sum r^{-l_i} \leq 1\). Conversely, given any set of natural numbers \(l_i\) satisfying the inequality, there exists a uniquely decodable code over an alphabet of size \(r\) with those codeword lengths. Essentially we can show that any uniquely decodable code would satisfy the Kraft inequality, which allows us to construct a prefix code with equivalent codeword lengths. That is really neat and we can focus on optimising prefix codes to maximise data compression without any loss of generality!</p>

<hr />

<p>We can represent any binary coding scheme with a binary tree, labelling each leaf as an encoded character (refer to diagrams in this section). Then for each character \(c\) in the alphabet \(C\), the number of bits \(B(T)\) required for encoding (i.e. cost) is</p>

\[B(T) = \sum_{c\in C} c.\text{freq}\cdot \text{depth}(c).\]

<p>Intuitively, we want to give the longer codes to the lower-frequency characters, and the Huffman encoding does so optimally. The algorithm greedily merges the lowest-frequency characters (nodes), and returns the binary tree representing the prefix code:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// Maintain a frequency f for each node v, which is the sum
// of frequencies of all leaves in the subtree rooted at v.
// Let Q be a min-priority queue, keyed on frequency
for i = 1 to |C|:
    create new node z
    x = EXTRACT-MIN(Q)
    y = EXTRACT-MIN(Q)
    z.left = x
    z.right = y
    insert z into Q
return EXTRACT-MIN(Q)
</code></pre></div></div>

<blockquote>
  <p><strong>Problem 16.3-2</strong>
Prove that a binary tree that is not full cannot correspond to an optimal prefix code.</p>
</blockquote>

<p>Proof:</p>

<p>A full binary tree is a binary tree in which all internal nodes have exactly 2 children. If any node has only one child, we can replace that node by the child subtree, producing a new valid prefix code with lower depths, thus lower cost.</p>

<hr />]]></content><author><name></name></author><category term="notes" /><category term="clrs" /><summary type="html"><![CDATA[A section on Huffman coding, and prefix codes in general]]></summary></entry><entry><title type="html">CLRS 16.2: Greedy algorithms</title><link href="https://wongkj12.github.io/blog/2023/greedy/" rel="alternate" type="text/html" title="CLRS 16.2: Greedy algorithms" /><published>2023-07-25T16:03:00+00:00</published><updated>2023-07-25T16:03:00+00:00</updated><id>https://wongkj12.github.io/blog/2023/greedy</id><content type="html" xml:base="https://wongkj12.github.io/blog/2023/greedy/"><![CDATA[<blockquote>
  <p><strong>Problem 16.2-7</strong></p>

  <p>Suppose you are given two sets \(A\) and \(B\), each containing \(n\) positive integers.
You can choose to reorder each set however you like. After reordering, let \(a_i\) be
the \(i\)-th element of set \(A\), and let \(b_i\) be the \(i\)-th element of set \(B\).
You then receive a payoff of \(\prod_{i} a_i^{b_i}\). Give an algorithm that will
maximise your payoff.</p>
</blockquote>

<hr />

<p>Intuitively, it seems that sorting the sets \(A\) and \(B\) will maximise the payoff,
by giving the greater exponents to the greater bases. We can prove that this is optimal.</p>

<p>Let set \(A = {a_1,a_2,\ldots,a_n}\) be in sorted order. Suppose \(B\) is ordered such that
there is some \(i,j\) such that \(i&lt;j\) but \(b_i &gt; b_j\). Note that the product \(a_i^{b_i}{a_j}^{b_j}\) is a factor of the final payoff, and we can just try to improve this factor. If we swap the positions of \(b_i\) and \(b_j\), the new product \(a_i^{b_j}a_j^{b_i}\) changes by a factor
of \(a_i^{b_i-b_j}a_j^{b_j-b_i} &gt; a_i^{b_i-b_j}a_i^{b_j-b_i} = 1\), and thus increases.</p>

<p>If \(B\) is in sorted order, then no such \(i,j\) will exist, and we can no longer improve
the payoff by this process anymore. Thus sorting \(B\) must then provide the optimal payoff
(Noting that an optimal payoff clearly exists).</p>

<hr />

<blockquote>
  <p><strong>Problem 16.2-6</strong></p>

  <p>Show how to solve the fractional knapsack problem in \(O(n)\) time.</p>
</blockquote>

<p>I was quite surprised to learn that there is an \(O(n)\) solution, but it kind of makes
sense when comparing it to finding order statistics.</p>

<p>The classic \(0-1\) knapsack problem has a thief robbing a store with \(n\) items.
The \(i\)-th item is worth \(v_i\) dollars and weighs \(w_i\) pounds, where \(v_i\) and \(w_i\)
are integers. The thief wants to maximise the value of his load, but he can carry at most \(W\) pounds in his knapsack, where \(W\) is an integer.</p>

<p>In the fractional knapsack problem, the thief can take any fraction of each item. It is actually an easier problem, because we can greedily select the item with the best value-for-weight ratio, \(v_i/w_i\). Sorting the items by this ratio will take \(O(n\,log \,n)\).</p>

<p>However, intuitively we are not particularly interested in the <em>order</em> of the items, so sorting is extra effort. Since we can only carry \(W\) pounds, as long as a set of the highest value-for-weight items have total weight less than \(W\), we are always taking the whole set (regardless of order). I found the following solution online:</p>

<p>We can find the item of median value-for-weight in \(O(n)\) (using quickselect), and sum the weight of all items with value-for-weight greater than the median as \(M\). If \(M &gt; W\), then we know that the solution lies in taking items only among this collection, i.e. we are solving the fractional knapsack problem on input of size \(n/2\). If \(M &lt; W\), then we take all \(M\) pounds of high value-for-weight items, and solve the fractional knapsack problem again on the remaining \(n/2\) items. We have a runtime of \(T(n) = T(n/2) + O(n)\), which gives a total runtime of \(O(n)\).</p>]]></content><author><name></name></author><category term="notes" /><category term="clrs" /><summary type="html"><![CDATA[Problem 16.2-7 Suppose you are given two sets \(A\) and \(B\), each containing \(n\) positive integers. You can choose to reorder each set however you like. After reordering, let \(a_i\) be the \(i\)-th element of set \(A\), and let \(b_i\) be the \(i\)-th element of set \(B\). You then receive a payoff of \(\prod_{i} a_i^{b_i}\). Give an algorithm that will maximise your payoff.]]></summary></entry><entry><title type="html">CLRS 5.4: Probabilistic analysis and indicator random variables</title><link href="https://wongkj12.github.io/blog/2023/probabilistic-analysis/" rel="alternate" type="text/html" title="CLRS 5.4: Probabilistic analysis and indicator random variables" /><published>2023-07-20T16:03:00+00:00</published><updated>2023-07-20T16:03:00+00:00</updated><id>https://wongkj12.github.io/blog/2023/probabilistic-analysis</id><content type="html" xml:base="https://wongkj12.github.io/blog/2023/probabilistic-analysis/"><![CDATA[<blockquote>
  <p><strong>Problem 5.4-6</strong></p>

  <p>Suppose that \(n\) balls are tossed into \(n\) bins, where each toss is independent and the
ball is equally likely to end up in any bin. What is the expected number of empty bins? What is
the expected number of bins with exactly one ball?</p>
</blockquote>

<hr />

<p>Let our indicator random variable be \(X_i\), the event that bin \(i\) is empty.
\(P(X_i) = (1-\frac{1}{n})^n.\) Then our desired expectation is</p>

\[E(X) = \sum_{i} E(X_i) = n(1-\frac{1}{n})^n.\]

<p>Similarly, let \(Y_i\) be the indicator random variable denoting the event that bin \(i\)
has exactly one ball. \(P(Y_i) = (1-\frac{1}{n})^{n-1}\). Then the expectation is</p>

\[E(Y) = \sum_{i} E(Y_i) = n(1-\frac{1}{n})^{n-1}\]

<p>Interestingly, since \((1+\frac{x}{n})^n \approx e^x\), both expectations actually
tend to the same value, \(\frac{n}{e}\) when \(n\) is large.</p>

\[\]]]></content><author><name></name></author><category term="notes" /><category term="clrs" /><summary type="html"><![CDATA[More probabilistic problems]]></summary></entry><entry><title type="html">CLRS Appendix C: Tails of the binomial distribution (in progress)</title><link href="https://wongkj12.github.io/blog/2023/binomialtails/" rel="alternate" type="text/html" title="CLRS Appendix C: Tails of the binomial distribution (in progress)" /><published>2023-07-11T16:23:43+00:00</published><updated>2023-07-11T16:23:43+00:00</updated><id>https://wongkj12.github.io/blog/2023/binomialtails</id><content type="html" xml:base="https://wongkj12.github.io/blog/2023/binomialtails/"><![CDATA[<p>The <strong>tails</strong> of the binomial distribution refers to the two regions of the distribution \(b(k;n,p)\) that are far from the mean \(np\).</p>

<p>First, we may provide bounds on the right tail of \(b(k;n,p)\). Let \(X\) be a random variable denoting the number of successes in \(n\) Bernoulli trials with probability of success \(p\), i.e. \(X \sim Bin(n,p)\). Then for \(0\leq k \leq n\),</p>

\[\begin{aligned}
P(X\geq k) &amp;= \sum_{i=k}^{n} b(i;n,p) \\
&amp;\leq {n\choose k} p^k. \\
\end{aligned}\]

<p><strong>Proof:</strong></p>

<p>Clearly, whenever at least \(k\) trials are successes, we have \(X\geq k\). The probability of exactly \(k\) trials being a successes is \(p^k\), and there are \({n\choose k}\) possible \(k\)-subsets of such trials. However, when we sum this together to \({n\choose k}p^k\), we are “overcounting” the probabilities here, as any \(k\)-subset of trials being successes are not neccessarily disjoint events, which gives us an upper bound. More formally, we are using the union bound here.</p>

<p>Similarly, we can make the same statement about the left tail of \(b(k;n,p)\):</p>

\[\begin{aligned}
P(X\leq k) &amp;= \sum_{i=0}^k b(i;n,p) \\
&amp;\leq {n \choose {n-k}} (1-p)^{n-k} \\
&amp;= {n \choose k}(1-p)^{n-k} \\
\end{aligned}\]

<p><strong>Proof</strong></p>

<p>By a similar union-bound argument, whenever at least \(n-k\) trials are failures, we have \(X\leq k\).</p>

<hr />]]></content><author><name></name></author><category term="notes" /><category term="clrs" /><category term="math" /><summary type="html"><![CDATA[The tails of the binomial distribution]]></summary></entry><entry><title type="html">CLRS Appendix B: More graph theory, and Dirac’s theorem</title><link href="https://wongkj12.github.io/blog/2023/moregraphtheory/" rel="alternate" type="text/html" title="CLRS Appendix B: More graph theory, and Dirac’s theorem" /><published>2023-07-03T16:23:43+00:00</published><updated>2023-07-03T16:23:43+00:00</updated><id>https://wongkj12.github.io/blog/2023/moregraphtheory</id><content type="html" xml:base="https://wongkj12.github.io/blog/2023/moregraphtheory/"><![CDATA[<blockquote>
  <p><strong>Problem B-2 (c) (rephrased)</strong></p>

  <p>Given any graph \(G=(V,E)\), the vertex set \(V\) can be partitioned into two subsets \(V_1,V_2\) such that at least half the neighbours of any \(u\in V_1\) are in \(V_2\), and at least half the neighbours of any \(v\in V_2\) are in \(V_1\).</p>
</blockquote>

<hr />

<p>Note that if all of the neighbours of any \(u\in V_1\) are in \(V_2\) and vice-versa, this is just a 2-coloring of the graph, and \(G\) would be bipartite. It is kind of interesting to draw the partitioning of \(G\) as some kind of bipartite-looking graph. In fact, I think the edges that cross from \(V_1\) to \(V_2\) must at least be twice the number of edges from \(V_1\) to \(V_1\), or \(V_2\) to \(V_2:\)</p>

<p>Let the number of edges that cross \(V_1\) to \(V_2\) be \(E_{12}\), the number of edges only within \(V_1\) be \(E_1\), and similarly define \(E_2\). Note that \(E_1 = \frac{1}{2}((\sum_{u\in V_1}deg(u)) - E_{12})\) (by handshaking lemma). Then</p>

\[\begin{aligned}
E_{12} &amp;\geq \frac{1}{2} \sum_{u\in V_1} deg(u) \\
&amp;\geq \frac{1}{2} E_{12} + E_1 \\
\end{aligned}\]

<p>Thus \(E_{12} \geq 2E_1\), and similarly \(E_{12} \geq 2E_2\). However, I did not find this approach useful in proving the statement.</p>

<p><strong>Proof</strong>:</p>

<p>Arbitrarily try to assign vertices to either \(V_1\) or \(V_2\). If all vertices meet the condition, then we are done. Otherwise, pick a vertex which does not satisfy the condition, and swap the subset it belongs to such that it now satisfies the condition. Note that the number of edges crossing from \(V_1\) to \(V_2\) strictly increases this way. If \(\lvert E\rvert\) is finite, then this process must terminate, resulting in the desired partition.</p>

<blockquote>
  <p><strong>Problem B-2 (d) (rephrased, i.e. Dirac’s Theorem)</strong></p>

  <p>Given any graph \(G=(V,E)\), with \(n \geq 3\) vertices, if each vertex has degree at least \(n/2\), then \(G\) contains a Hamiltonian cycle (A cycle which visits each vertex exactly once).</p>
</blockquote>

<hr />

<p>First, we show that a longest simple path in \(G\) is a Hamiltonian path (A path which visits each vertex exactly once). Let such a path be \(P = (v_1,v_2,\ldots,v_k)\). Suppose there is a vertex \(u\) in \(G\) that is outside of \(P\). We proceed by showing that there is a vertex \(v_j\) on \(P\) that is a neighbour of \(v_k\), such that \(v_{j+1}\) is a neighbour of \(u\). Since \(P\) is a longest path, all neigbours of \(v_k\) must be on \(P\). Thus there at at least \(n/2\) neighbours \(v_i\) of \(v_k\) on \(P\), and we mark each of the \(n/2\) neighbours \(v_{i+1}\) on \(P\). Now, since \(u\) has at least \(n/2\) neighbours, by the Pigeonhole Principle, it is a neighbour of at least one of these marked vertices. Thus the vertex \(v_j\) exists, and we can construct a longer path \((v_1,v_2,\ldots,v_j,v_k,v_{k-1},\ldots,v_{j+1},u)\) (contradiction). Thus \(P\) must be a Hamiltonian path.</p>

<p>Next, given \(P = (v_1,\ldots,v_n)\) we can construct a Hamiltonian cycle. Using a similar argument, we can show that there exists a pair \((v_t,v_{t+1})\) on \(P\) such that \(v_t\) is a neighbour of \(v_n\) and \(v_{t+1}\) is a neighbour of \(v_1\). Then we can rearrange vertices to form the cycle \((v_1,v_2,\ldots,v_t,v_n,v_{n-1},\ldots,v_{t+1},v_1)\).</p>]]></content><author><name></name></author><category term="notes" /><category term="clrs" /><category term="math" /><summary type="html"><![CDATA[Two more graph theory problems]]></summary></entry><entry><title type="html">CLRS Appendix B: A theorem on graph coloring</title><link href="https://wongkj12.github.io/blog/2023/graphcoloringtheorem/" rel="alternate" type="text/html" title="CLRS Appendix B: A theorem on graph coloring" /><published>2023-07-02T16:01:19+00:00</published><updated>2023-07-02T16:01:19+00:00</updated><id>https://wongkj12.github.io/blog/2023/graphcoloringtheorem</id><content type="html" xml:base="https://wongkj12.github.io/blog/2023/graphcoloringtheorem/"><![CDATA[<blockquote>
  <p><strong>Problem B-1 (d)</strong></p>

  <p>Show that if a graph \(G\) has \(O(V)\) edges, then we can color \(G\) with \(O(\sqrt{V})\) colors.</p>
</blockquote>

<hr />

<p>Here is a proof by trying the greedy algorithm:</p>

<p><strong>Proof</strong>:</p>

<p>Attempt to color the graph greedily (i.e. color an arbitrary vertex, then start coloring its neighbours, using new colors whenever needed, and so on). Whenever we need to use a new color \(n\) on a vertex, that vertex must neccessarily have edges connecting it to vertices colored in each one of the previous \(n-1\) colors. Everytime we use a new color, mark all of these edges. Every edge is marked at most once, and so if we end the algorithm using \(c\) colors, at least \(1+2+\ldots+c = \frac{c(c+1)}{2}\) edges must have been marked. Thus \(\frac{c(c+1)}{2} \leq E\), so \(\frac{c(c+1)}{2} \in O(V)\), and we have that \(c \in O(\sqrt{V})\).</p>]]></content><author><name></name></author><category term="notes" /><category term="clrs" /><category term="math" /><summary type="html"><![CDATA[An exercise on graph coloring from Appendix B]]></summary></entry><entry><title type="html">CLRS Appendix C: Comparing sums of Bernoulli trials</title><link href="https://wongkj12.github.io/blog/2023/comparing-bernoulli/" rel="alternate" type="text/html" title="CLRS Appendix C: Comparing sums of Bernoulli trials" /><published>2023-06-29T16:20:48+00:00</published><updated>2023-06-29T16:20:48+00:00</updated><id>https://wongkj12.github.io/blog/2023/comparing-bernoulli</id><content type="html" xml:base="https://wongkj12.github.io/blog/2023/comparing-bernoulli/"><![CDATA[<p>Here is an interesting exercise on probability distributions.</p>

<blockquote>
  <p><strong>Exercise C.4-9</strong></p>

  <p>Let \(X\) be the random variable for the total number of successes in a set \(A\) of \(n\) Bernoulli trials, where the \(i\)th trial has a probability \(p_i\) of success, and let \(X'\) be the random variable for the total number of successes in a second set \(A'\) of \(n\) Bernoulli trials, where the \(i\)th trial has a probability \(p_i' \geq p_i\) of success. Prove that for \(0\leq k\leq n\),</p>

  <p>\(P(X'\geq k) \geq P(X\geq k)\).</p>

  <p>(<em>Hint:</em> Show how to obtain the Bernoulli trials in \(A'\) by an experiment involving the trials of \(A\), and use the result of Exercise C.3-7.)</p>
</blockquote>

<hr />

<p>I found this exercise interesting as the result is intuitive, but it’s tricky to find a way to “compare” the distributions, as per the hint.</p>

<p>This is one clever solution I found online, which seems obvious in hindsight:</p>

<p>Consider comparing each Bernoulli trial to picking a random value \(S\) on the unit interval. If \(S \leq p_i\), we can call that a success in set \(A\). If \(S \leq p_i'\), we can call that a success in set \(A'\). If \(S \leq p_i\), then \(S \leq p_i'\). Then clearly, whenever there is a success for set \(A\), there must be a success for set \(A'\). So \(X' \geq X\), and \(P(X' \geq k) \geq P(X \geq k)\).</p>]]></content><author><name></name></author><category term="notes" /><category term="clrs" /><category term="math" /><summary type="html"><![CDATA[On comparing sums of Bernoulli trials]]></summary></entry><entry><title type="html">CLRS Appendix B: Three problems on trees</title><link href="https://wongkj12.github.io/blog/2023/treeproblems/" rel="alternate" type="text/html" title="CLRS Appendix B: Three problems on trees" /><published>2023-06-29T16:20:48+00:00</published><updated>2023-06-29T16:20:48+00:00</updated><id>https://wongkj12.github.io/blog/2023/treeproblems</id><content type="html" xml:base="https://wongkj12.github.io/blog/2023/treeproblems/"><![CDATA[<p>Here are three problems on trees from Appendix B.</p>

<blockquote>
  <p><strong>Exercise B.5-3</strong></p>

  <p>Show that the number of degree-2 nodes in any nonempty binary tree is 1 fewer than the number of leaves. Conclude that the number of internal nodes in a full binary tree is 1 fewer than the number of leaves.</p>
</blockquote>

<p>Note that in the context of a binary tree, the “degree-2” nodes refer to nodes with exactly 2 <em>children</em>.</p>

<p><strong>Proof by strong induction</strong>: Let the number of degree-2 nodes in any tree by \(D\), and the number of leaves be \(L\). Let the number of nodes in a tree be \(n\). Clearly, \(D=L-1\) when \(n=1\). Suppose it is true for all trees with \(\leq n\) nodes. Consider the root of a tree with \(n+1\) nodes. Letting subscripts \(l\) and \(r\) denote the left and right subtrees, we have that</p>

\[\begin{aligned}
D &amp;= D_l + D_r + 1 \\
&amp;= (L_l + L_r - 2) + 1 \\
&amp;= (L_l + L_r) - 1 \\
&amp;= L - 1.
\end{aligned}\]

<p>In a full binary tree, all internal nodes are of degree-2.</p>

<hr />

<blockquote>
  <p><strong>Exercise B.5-6</strong></p>

  <p>Let us associate a “weight” \(w(x)=2^{-d}\) with each leaf \(x\) of depth \(d\) in a binary tree \(T\), and let \(L\) be the set of leaves on \(T\). Prove that \(\sum_{x\in L} w(x) \leq 1.\) (This is known as the <strong>Kraft inequality</strong>.)</p>
</blockquote>

<p><strong>Proof by strong induction</strong>: Let the number of nodes in any tree \(T\) be \(n\). Clearly when \(n=1\), \(\sum_{x\in L}w(x) = 2^0 \leq 1.\) Suppose \(\sum_{x\in L}w(x) \leq 1\) for all trees with \(\leq n\) nodes. Consider the root of a tree \(T\) with \(n+1\) nodes. Notice that the depth of all leaves in the left and right subtrees increase by 1 relative to the root. Similarly letting subscripts \(l\) and \(r\) denote the left and right subtrees, we have that</p>

\[\begin{aligned}
\sum_{x\in L}w(x) &amp;= \frac{1}{2}\sum_{x\in L_l}w(x) + \frac{1}{2}\sum_{x\in L_r}w(x) \\
&amp;\leq \frac{1}{2}(1) + \frac{1}{2}(1) \\
&amp;\leq 1.
\end{aligned}\]

<hr />

<blockquote>
  <p><strong>Exercise B.5-7</strong></p>

  <p>Show that if \(L\geq 2\), then every binary tree with \(L\) leaves contains a subtree having between \(L/3\) and \(2L/3\) leaves, inclusive.</p>
</blockquote>

<p><strong>Proof by contradiction</strong>: Suppose that there is a binary tree with \(L\) leaves, with no subtrees having between \(L/3\) and \(2L/3\) leaves. If \(L\geq 2\), then the root of this tree cannot be a leaf. Take the path \(x_0 = root, x_1, \ldots , x_k\) on the tree down to a leaf, at each step picking the child subtree with the larger number of leaves. Consider the number of leaves of the subtree rooted at each \(x_i\). Clearly the number of leaves is a decreasing sequence, so along the path the number of leaves must decrement from \(x_i\) with more than \(2L/3\) leaves to \(x_{i+1}\) with less than \(L/3\) leaves. However, if \(x_i\) has less than \(L/3\) leaves, then \(x_{i+1}\) must have had less than \(2L/3\) leaves, a contradiction.</p>]]></content><author><name></name></author><category term="notes" /><category term="clrs" /><category term="math" /><summary type="html"><![CDATA[Three tree problems from Appendix B]]></summary></entry><entry><title type="html">CLRS Appendix B: Equivalent conditions for trees</title><link href="https://wongkj12.github.io/blog/2023/trees/" rel="alternate" type="text/html" title="CLRS Appendix B: Equivalent conditions for trees" /><published>2023-06-26T16:01:20+00:00</published><updated>2023-06-26T16:01:20+00:00</updated><id>https://wongkj12.github.io/blog/2023/trees</id><content type="html" xml:base="https://wongkj12.github.io/blog/2023/trees/"><![CDATA[<p>A <em>tree</em> is a connected, acyclic (undirected) graph. Let \(G = (V,E)\) be an undirected graph. Here are some equivalent conditions for \(G\) to be a tree:</p>

<ol>
  <li>\(G\) is a tree.</li>
  <li>Any two vertices in \(G\) are connected by a unique simple path.</li>
  <li>\(G\) is connected, but if any edge is removed from \(E\), the resulting graph is disconnected.</li>
  <li>\(G\) is connected, and \(\lvert E\rvert= \lvert V\rvert-1\).</li>
  <li>\(G\) is acyclic, and \(\lvert E\rvert = \lvert V\rvert-1\).</li>
  <li>\(G\) is acyclic, but if any edge is added to \(E\), the resulting graph contains a cycle.</li>
</ol>

<hr />

<p>I will roughly prove their equivalence here.</p>

<p>\((1) \Rightarrow (2)\):
Since \(G\) is connected, any two vertices \(u, v\) are connected by at least one simple path. If there exists more than one such path between \(u\) and \(v\), then \(G\) contains a cycle (Contradiction).</p>

<p>\((2) \Rightarrow (3)\):
Suppose we remove edge \((u,v)\). If there still exists a path between \(u\) and \(v\), then there would have more than one simple path connecting \(u\) and \(v\) initially (Contadiction). Thus \(u\) and \(v\) must not be connected after the removal of \((u,v)\) so the resulting graph is disconnected.</p>

<p>\((3) \Rightarrow (4)\):
Prove by strong induction (see (A) below).</p>

<p>\((4) \Rightarrow (5)\):
Suppose \(G\) contains cycles. Remove edges from cycles in \(G\) until we get an acyclic graph \(G'=(E',V')\). Note that \(G'\) is still connected, so \(\lvert E'\rvert \geq \lvert V'\rvert - 1\). However, \(\lvert E\rvert = \lvert V\rvert -1\), so this implies we have removed 0 edges! Thus \(G\) must have been acyclic$$.</p>

<p>\((5) \Rightarrow (6)\):
See (B) below.</p>

<p>\((6) \Rightarrow (1)\):
Suppose there is no path connecting \(u\) and \(v\) in \(G\). Then add the edge \((u,v)\) to \(G\). This new edge cannot have formed a cycle (Contradiction). Thus \(G\) must have been connected initially.</p>

<hr />

<p>Additionally, here are two more useful facts: (can be proven via strong induction)</p>

<p>A. \(G\) is connected \(\Rightarrow \lvert E\rvert \geq \lvert V\rvert-1\).</p>

<p>B. \(G\) is acyclic \(\Rightarrow  \lvert E\rvert \leq \lvert V\rvert-1\).</p>]]></content><author><name></name></author><category term="notes" /><category term="clrs" /><category term="math" /><summary type="html"><![CDATA[On trees]]></summary></entry><entry><title type="html">CLRS Appendix C: Some probability and bounds</title><link href="https://wongkj12.github.io/blog/2023/probability/" rel="alternate" type="text/html" title="CLRS Appendix C: Some probability and bounds" /><published>2023-05-08T16:03:50+00:00</published><updated>2023-05-08T16:03:50+00:00</updated><id>https://wongkj12.github.io/blog/2023/probability</id><content type="html" xml:base="https://wongkj12.github.io/blog/2023/probability/"><![CDATA[<p>An interesting problem (simulating an unbiased coin given a biased one):</p>

<blockquote>
  <p><strong>Exercise 5.1-3 (rephrased)</strong></p>

  <p>Suppose you want to output 0 with probability 1/2 and 1 with probability 1/2.
You have a biased function \(R\) which outputs 1 with probability \(p\), and 0 with probability
\(1-p\), where \(0 &lt; p &lt; 1\). Give an algorithm which uses \(R\) to output
an unbiased answer. What is the expected running time?</p>
</blockquote>

<p>Call \(R\) twice and store the results in variables \(x\) and \(y\). Note that since \(x\) and \(y\)
are independent and have the same distribution, by symmetry, \(P(x &lt; y) = P(x &gt; y)\), so we just output 1 whenever
\(x &gt; y\), 0 if \(x &lt; y\), and repeat otherwise. The probability of termination in one iteration is constant, equal to
\(P(x \neq y) = 2p(1-p)\), so the number of iterations until termination follows a geometric distribution. Thus in expectation,
the number of iterations will be \(\frac{1}{2p(1-p)}\).</p>

<hr />

<p>While I learn probability, here I’ll list some interesting bounds/formulas from Appendix C:</p>

<p><strong>Stirling’s approximation (3.18)</strong></p>

\[n! = \sqrt{2\pi n}\left(\frac{n}{e}\right)^n (1+\Theta(1/n)).\]

<p><strong>Binomial bounds</strong></p>

\[{n\choose k} = \left(\frac{n}{k}\right)\left(\frac{n-1}{k-1}\right)\cdots\left(\frac{n-k+1}{1}\right) \geq \left(\frac{n}{k}\right)^k.\]

\[{n\choose k} \leq \frac{n^k}{k!} \leq \left(\frac{en}{k}\right)^k.\]

\[{n\choose k} \leq \frac{n^n}{k^k (n-k)^{n-k}}.\]

<p><strong>Boole’s inequality/Union bound</strong></p>

\[P\left(\bigcup_i A_i\right) \leq \sum_i P(A_i).\]

<p><strong>Jensen’s inequality (Probabilistic form)</strong></p>

<p>If \(X\) is a random variable and \(f\) is a <em>convex</em> function (i.e. for all \(x,y\) and all \(0 \leq \lambda \leq 1\), we have
\(f(\lambda x + (1-\lambda)y) \leq \lambda f(x) + (1-\lambda)f(y)\)), then</p>

\[\mathbb{E}(f(X)) \geq f(\mathbb{E}(X)),\]

<p>provided that the expectations exist and are finite.</p>

<p><strong>Markov’s inequality (Exercise C.3-6)</strong></p>

<p>Let X be a nonnegative random variable, and suppose that \(\mathbb{E}(X)\) is well-defined. Then</p>

\[P(X\geq t) \leq \mathbb{E}(X)/t\]

<p>for all \(t&gt;0\). To see this note that</p>

\[\begin{aligned}
\mathbb{E}(X) &amp;= \int_{0}^{\infty} xP(X=x)\, dx \\
&amp;= \int_{0}^{t} xP(X=x)\, dx + \int_{t}^{\infty} xP(X=x)\, dx \\
&amp;\geq \int_{t}^{\infty} xP(X=x)\, dx \\
&amp;\geq \int_{t}^{\infty} tP(X=x)\, dx \\
&amp;\geq t\int_{t}^{\infty} P(X=x)\, dx \\
&amp;\geq tP(X&gt;t).
\end{aligned}\]]]></content><author><name></name></author><category term="notes" /><category term="clrs" /><category term="math" /><summary type="html"><![CDATA[Some probability and useful bounds]]></summary></entry></feed>